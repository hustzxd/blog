---
title: paper-of-quantization
date: 2019-01-11 17:20:22
tags:
	- paper
	- quantization
---

# Related papers
1. Deep learning with limited numerical precision. **2015 IBM**
2. **DoReFa-Net**: Training low bit-width convolutional neural networks with low bit-width gradients. **2016**
3. **XNOR-Net**: ImageNet Classification using binary convolutional neural networks. **ECCV2016 washington**
4. **BNN**: Binarized Neural Networks. **NIPS2016**
5. Fixed point quantization of deep convolutional networks. **2016**
6. Hardware-oriented approximation of convolutional neural networks. **ICLR2016**
7. **TWNs**: Ternary weight networks. **NIPS2016** ucas
8. Quantized convolutional neural networks for mobile devices. **CVPR2016** nlpr
9. **Flexpoint**: an adaptive numerical format for efficient training of deep neural networks. **2017** intel
10. **INQ**: Incremental network quantization, towards lossless CNNs with low-precision weights. **ICLR2017** intel labs china
11. **TTQ**: Trained ternary quantization. **ICLR2017** stanford
12. **WRPN**: wide reduced-precision networks. **2017** [detailed]()
41. A Survey of Model Compression and Acceleration for Deep Neural Networks. **201712**
41. **HWGQ**: Deep Learning with Low Precision by Half-wave Gaussian Quantization. **CVPR2017**
13. **VNQ**: Variational network quantization. **ICLR2018**
14. **WAGE**: Training and Inference with Integers in Deep Neural Networks. **ICLR2018** oral tsinghua
15. **Clip-Q**: Deep network compression learning by In-Parallel Pruning Quantization. **CVPR2018** SFU
16. **LQ-NETs**: learned quantization for highly accurate and compact deep neural networks. **ECCV2018** Microsoft
17. **Bi-Real Net**: Enhancing the performance of 1-bit CNNs with improved Representational capability and advanced training algorithm. **ECCV2018** HKU
18. **Synergy**: Algorithm-hardware co-design for convnet accelerators on embedded FPGAs. **2018** UC Berkeley
19. Alternating multi-bit quantization for recurrent neural networks. **ICLR2018** alibaba
20. Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware. **2018**
21. **ELQ**: Explicit loss-error-aware quantization for low-bit deep neural networks. **CVPR2018** intel tsinghua
22. From Hashing to CNNs: training Binary weights vis hashing. **AAAI2018** nlpr
23. **HAQ**: Hardware-Aware automated quantization. **NIPS workshop2018** mit
24. Heterogeneous Bitwidth Binarization in Convolutional Neural Networks. **NIPS2018** microsoft
25. **HALP**: High-Accuracy Low-Precision Training. **2018** stanford
26. Mixed Precision Training. **ICLR2018** baidu
27. **PACT**: parameterized clipping activation for quantized neural networks. **2018** IBM
28. Model Compression via distillation and quantization. **ICLR2018** google
29. Quantization and training of neural networks for efficient integer-arithmetic-only inference. **CVPR2018** Google
29. Towards Effective Low-bitwidth Convolutional Neural Networks. **CVPR2018**
30. Quantized back-propagation: training binarized neural networks with quantized gradients. **ICLR2018**
31. **QUENN**: Quantization engine for low-power neural networks. **CF18 ACM**
32. Scalable methods for 8-bits training of neural networks. **NIPS2018** intel
33. **SYQ**: learning symmetric quantization for efficient deep neural networks. **CVPR2018** xilinx
34. **TSQ**: two-step quantization for low-bit neural networks. **CVPR2018**
35. **V-Quant**: Value-aware quantization for training and inference of neural networks. **ECCV2018** facebook
36. **UNIQ**: Uniform noise injection for non-uniform quantization of neural networks. **2018**
37. Training a binary weight object detector by knowledge transfer for autonomous driving. **2018**
38. Training competitive binary neural networks from scratch. **2018**
39. **A white-paper**: Quantizing deep convolutional networks for efficient inference. **2018** google
40. **ACIQ**: analytical clipping for integer quantization of neural networks. **ICLR2019** Intel
42. Per-Tensor Fixed-point quantization of the back-propagation algorithm. **ICLR2019**
42. **RQ**: Relaxed Quantization for disretized NNs. **ICLR2019**
43. **SAWB**: Accurate and efficient 2-bit quantized neural networks. **sysml2019**
44. **SQuantizer**: Simultaneous Learning for Both Sparse and Low-precision Neural Networks. **2019** AIPG, Intel
45. Low-bit Quantization of Neural Networks for EfÔ¨Åcient Inference. **2019** huawei
46. **Post training** 4-bit quantization of convolution networks for rapid-deployment. **2019**
47. **FQN**: Fully Quantized Network for Object Detection. **CVPR2019** 
